{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import requests\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "import h3\n",
    "import git\n",
    "from geopy.distance import geodesic\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "from agent_components.llms.chatAI import ChatAIHandler\n",
    "import re"
   ],
   "id": "c668dc82a4c39a47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\data\\processed_GeoCoDe_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    geocode_test_set = json.load(f)"
   ],
   "id": "745c031d63be3871",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\n",
    "        fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\output\\reflective_candidate_resolution\\fatal_error_and_invalid_correction\\GeoCoDe\\meta-llama-3.1-8b-instruct_with_meta-llama-3.1-8b-instruct_critic\\20250122_seed_24_1000_articles\\correct_articles_k_161.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    correctly_geocoded_articles = json.load(f)\n",
    "print(len(correctly_geocoded_articles))\n",
    "correctly_geocoded_articles"
   ],
   "id": "e9a251ed9a346216",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "geo_relatable_harsh_from_geocoded_articles = [\n",
    "    article for article in correctly_geocoded_articles\n",
    "    if (any(word in article[\"article_text\"] for word in [\"lies\", \"located\"]) and\n",
    "        any(word in article[\"article_text\"] for word in [\"km\", \"kilometer\", \"kilometre\"]))\n",
    "]\n",
    "print(len(geo_relatable_harsh_from_geocoded_articles))\n",
    "print(f\"# Toponyms: {sum(len(article['toponyms']) for article in geo_relatable_harsh_from_geocoded_articles)}\")\n",
    "geo_relatable_harsh_from_geocoded_articles"
   ],
   "id": "263f354e4cd2990a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run LLM Georelating for all fully correctly geocoded georelatable articles",
   "id": "df623665f73a12e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\n",
    "        fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\output\\reflective_candidate_resolution\\fatal_error_and_invalid_correction\\GeoCoDe\\meta-llama-3.1-8b-instruct_with_meta-llama-3.1-8b-instruct_critic\\20250122_seed_24_1000_articles\\articles_with_at_least_one_correct_toponym_k_161.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    correctly_geocoded_articles = json.load(f)"
   ],
   "id": "f89f9d851b7e184c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def georelate(model_name, long_term_memory, article_text, mentioned_toponyms):\n",
    "    prompt = long_term_memory.generate_georelating_prompt(article_text=article_text,\n",
    "                                                          mentioned_toponyms=mentioned_toponyms,\n",
    "                                                          example_path=fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\data\\few_shot_example_georelation.json\")\n",
    "    handler = ChatAIHandler()\n",
    "    model = handler.get_model(model_name)\n",
    "    llm_answer = model.invoke(prompt)\n",
    "    pattern = re.compile(r\"<think>(.*?)</think>\", re.DOTALL)\n",
    "\n",
    "    thoughts = pattern.findall(llm_answer.content)  # Extract thoughts\n",
    "    non_thoughts = pattern.sub(\"\", llm_answer.content)  # Remove thoughts from text\n",
    "    start = non_thoughts.find(\"```json\")\n",
    "    if start != -1:\n",
    "        start = start + 7\n",
    "        end = non_thoughts.find(\"```\", start)  # Find the next occurrence after start\n",
    "        content = non_thoughts[start:end]\n",
    "        prediction = json.loads(content)\n",
    "    elif non_thoughts.find(\"```\") != -1:\n",
    "        start = start + 4\n",
    "        end = non_thoughts.find(\"```\", start)  # Find the next occurrence after start\n",
    "        content = non_thoughts[start:end]\n",
    "        prediction = json.loads(content)\n",
    "    else:\n",
    "        prediction = json.loads(llm_answer.content)\n",
    "    return thoughts, prediction"
   ],
   "id": "5df77d3880d1c1cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Add ground truth center coordinates, bounding box and approx area to articles",
   "id": "eed7c2e6115e23da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_bounding_box(relation_id):\n",
    "    query = f\"\"\"\n",
    "    [out:json];\n",
    "    relation({relation_id});\n",
    "    out geom;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://overpass-api.de/api/interpreter\"\n",
    "        response = requests.post(url, data={\"data\": query})\n",
    "        data = response.json()\n",
    "\n",
    "        if not data['elements']:\n",
    "            return None, None, None, None\n",
    "\n",
    "        relation = data['elements'][0]\n",
    "        if 'bounds' not in relation:\n",
    "            return None, None, None, None\n",
    "\n",
    "        bounds = relation['bounds']\n",
    "        min_lat = bounds['minlat']\n",
    "        max_lat = bounds['maxlat']\n",
    "        min_lon = bounds['minlon']\n",
    "        max_lon = bounds['maxlon']\n",
    "\n",
    "        return min_lat, max_lat, min_lon, max_lon\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching relation {relation_id}: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def get_osm_admin_center(osm_id):\n",
    "    \"\"\"\n",
    "    Query Overpass API for the admin_center or center of the given OSM ID.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    [out:json];\n",
    "    relation({osm_id});\n",
    "    out center;\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://overpass-api.de/api/interpreter\"\n",
    "    response = requests.get(url, params={\"data\": query})\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        elements = data.get(\"elements\", [])\n",
    "\n",
    "        for element in elements:\n",
    "            if \"center\" in element:\n",
    "                return element[\"center\"]  # Returns {lat, lon}\n",
    "\n",
    "    return None  # Return None if no center/admin_center is found\n",
    "\n",
    "def get_area_sq_km(geometry, source_crs=\"EPSG:4326\", target_crs=\"EPSG:3857\"):\n",
    "    project = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=True).transform\n",
    "    projected_geometry = transform(project, geometry)\n",
    "    area_sq_km = projected_geometry.area / 1_000_000\n",
    "    return area_sq_km\n",
    "\n",
    "\n",
    "def process_articles(articles):\n",
    "    \"\"\"\n",
    "    Extracts the last ID from article_id and fetches its coordinates.\n",
    "    \"\"\"\n",
    "    processed_articles = []\n",
    "    results = {}\n",
    "\n",
    "    for article in articles:\n",
    "        article_ids = article[\"article_id\"].split()\n",
    "        last_id = article_ids[-1]  # Get last ID\n",
    "\n",
    "        centroid_coords = get_osm_admin_center(last_id)\n",
    "\n",
    "        bb_min_lat, bb_max_lat, bb_min_lon, bb_max_lon = get_bounding_box(last_id)\n",
    "        if all((bb_min_lat, bb_max_lat, bb_min_lon, bb_max_lon)):\n",
    "            bounding_box_polygon = Polygon([(bb_min_lon, bb_min_lat), (bb_min_lon, bb_max_lat),\n",
    "                                        (bb_max_lon, bb_max_lat), (bb_max_lon, bb_min_lat)])\n",
    "\n",
    "            bb_area = get_area_sq_km(bounding_box_polygon)\n",
    "            if centroid_coords and bb_area:\n",
    "                article.update({'gt': {'centroid': centroid_coords,\n",
    "                                       'bounding_box':{\n",
    "                                           'bb_min_lat': bb_min_lat,\n",
    "                                           'bb_max_lat': bb_max_lat,\n",
    "                                           'bb_min_lon': bb_min_lon,\n",
    "                                           'bb_max_lon': bb_max_lon\n",
    "                                       },\n",
    "                                       'bb area': bb_area}})\n",
    "                processed_articles.append(article)\n",
    "\n",
    "    return processed_articles"
   ],
   "id": "89afd2451e6d5966",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "articles_for_georelation = process_articles(correctly_geocoded_articles)",
   "id": "33f504fe003f6343"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### H3: Resolution handling",
   "id": "b28462dc274fc553"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define H3 resolutions with their corresponding hexagon areas (from table)\n",
    "h3_data = [(res, h3.average_hexagon_area(res, unit=\"m^2\")) for res in range(16)]  # resolutions 0–15\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(h3_data, columns=[\"Resolution\", \"Hexagon_Area_km2\"])\n",
    "\n",
    "def get_h3_resolution_scaled(area_km2):\n",
    "    \"\"\"Find the H3 resolution where the input area and hexagon area are most proportionate.\"\"\"\n",
    "    df[\"Scale_Diff\"] = np.abs(np.log2(area_km2 / df[\"Hexagon_Area_km2\"]))\n",
    "    closest_resolution = df.loc[df[\"Scale_Diff\"].idxmin()]\n",
    "    return int(closest_resolution[\"Resolution\"])"
   ],
   "id": "983790e582437e0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parsing of Complex Location Descriptions Evaluation",
   "id": "5fffa928fdf84e21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_json_file(directory: str, search_string: str):\n",
    "    \"\"\"\n",
    "    Loops over all files in the specified directory and loads JSON files\n",
    "    whose filenames contain the specified search string.\n",
    "\n",
    "    :param directory: Path to the directory containing files.\n",
    "    :param search_string: String to search for in filenames.\n",
    "    :return: A dictionary with filenames as keys and loaded JSON content as values.\n",
    "    \"\"\"\n",
    "    json_data = None\n",
    "\n",
    "    if not os.path.isdir(directory):\n",
    "        raise ValueError(f\"The specified path '{directory}' is not a directory.\")\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if search_string in filename and filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                json_data = json.load(file)\n",
    "                break\n",
    "    return json_data\n",
    "\n",
    "GEORELATION_DIR = fr\"{git.Repo('.', search_parent_directories=True).working_tree_dir}˙\\output\\georelation\\all_articles_with_at_lest_one_correctly_geocoded_toponym\\llama-3.3-70b-instruct\\20250206\"\n",
    "k = 161\n",
    "\n",
    "all_error_distances = []\n",
    "all_squared_area_diffs = []\n",
    "all_log_q = []\n",
    "all_normalized_area_diffs = []\n",
    "evaluation_results = []\n",
    "very_off_area_predictions = []\n",
    "\n",
    "for gt_article in articles_for_georelation:\n",
    "    eval_results_for_article = {}\n",
    "    article_ids = gt_article[\"article_id\"].split()\n",
    "    last_id = article_ids[-1]  # Get last ID\n",
    "\n",
    "    # first, load generation\n",
    "    try:\n",
    "        georelated_article = load_json_file(GEORELATION_DIR, gt_article[\"article_id\"])\n",
    "        if not georelated_article:\n",
    "            for id in article_ids:\n",
    "                georelated_article = load_json_file(GEORELATION_DIR, id)\n",
    "                if georelated_article:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    if not georelated_article:\n",
    "        continue\n",
    "\n",
    "    # calculate distance of coordinates\n",
    "    gt_coordinates = (gt_article['gt']['centroid']['lat'],\n",
    "                      gt_article['gt']['centroid']['lon'])\n",
    "    predicted_coordinates = (georelated_article['georelation']['coordinates of geographical unit']['latitude'],\n",
    "                             georelated_article['georelation']['coordinates of geographical unit']['longitude'])\n",
    "    error_distance = geodesic(gt_coordinates, predicted_coordinates).kilometers\n",
    "    all_error_distances.append(error_distance)\n",
    "\n",
    "    # calculate area error distance\n",
    "    gt_area = gt_article['gt']['bb area']\n",
    "    predicted_area = georelated_article[\"georelation\"][\"area in square km\"]\n",
    "\n",
    "    log_q = np.log(predicted_area/gt_area)\n",
    "    all_log_q.append(log_q)\n",
    "\n",
    "    squared_area_diff = (predicted_area - gt_area)**2\n",
    "    all_squared_area_diffs.append(squared_area_diff)\n",
    "\n",
    "    normalized_error_diff = (predicted_area - gt_area) / gt_area\n",
    "    if normalized_error_diff > 10:\n",
    "        very_off_area_predictions.append(georelated_article)\n",
    "    all_normalized_area_diffs.append(normalized_error_diff)\n",
    "\n",
    "    # calculate intersection\n",
    "    # Define an H3 cell\n",
    "    h3_index = h3.latlng_to_cell(\n",
    "        lat=georelated_article[\"georelation\"][\"coordinates of geographical unit\"][\"latitude\"],\n",
    "        lng=georelated_article[\"georelation\"][\"coordinates of geographical unit\"][\"longitude\"],\n",
    "        res=get_h3_resolution_scaled(predicted_area)\n",
    "    )\n",
    "\n",
    "    # Get the boundary coordinates of the H3 cell\n",
    "    hexagon = h3.cell_to_boundary(h3_index)\n",
    "\n",
    "    # Convert polygon to shapely format\n",
    "    hexagon_coords = [(lng, lat) for lat, lng in hexagon]  # Convert to (lng, lat) for Folium\n",
    "    hexagon_polygon = Polygon(hexagon_coords)\n",
    "    hexagon_area_estimate = get_area_sq_km(hexagon_polygon)\n",
    "\n",
    "    bb = gt_article['gt']['bounding_box']\n",
    "    bb_min_lat, bb_max_lat, bb_min_lon, bb_max_lon = bb['bb_min_lat'], bb['bb_max_lat'], bb['bb_min_lon'], bb['bb_max_lon']\n",
    "    bounding_box_polygon = Polygon([(bb_min_lon, bb_min_lat), (bb_min_lon, bb_max_lat),\n",
    "                                    (bb_max_lon, bb_max_lat), (bb_max_lon, bb_min_lat)])\n",
    "    intersection = bounding_box_polygon.intersection(hexagon_polygon)\n",
    "    if not intersection.is_empty:\n",
    "        intersection_area = get_area_sq_km(intersection)\n",
    "    else:\n",
    "        intersection_area = 0\n",
    "\n",
    "    eval_results_for_article.update({\"article_id\": last_id,\n",
    "                                     \"gt\": gt_article['gt'],\n",
    "                                     \"georelation\": georelated_article['georelation'],\n",
    "                                     \"absolute_error_distance\": error_distance,\n",
    "                                     \"squared_area_error\": squared_area_diff,\n",
    "                                     \"log_q\": log_q,\n",
    "                                     \"normalized_area_error\": normalized_error_diff,\n",
    "                                     \"hexagon_area_estimate\": hexagon_area_estimate,\n",
    "                                     \"area_intersection\": intersection_area})\n",
    "    evaluation_results.append(eval_results_for_article)\n",
    "\n",
    "all_normalized_area_diffs = np.array(all_normalized_area_diffs)\n",
    "# Accuracy@k\n",
    "within_k = [d <= k for d in all_error_distances]\n",
    "accuracy_at_k = sum(within_k) / len(all_error_distances)\n",
    "print(f\"Accuracy@{k}: {accuracy_at_k}\")\n",
    "\n",
    "# AUC\n",
    "def calculate_auc(sorted_values):\n",
    "    max_error = 20038  # Earth's circumference in km / 2 (maximum possible distance)\n",
    "    size = len(sorted_values)\n",
    "    if size <= 1:\n",
    "        return 0.0\n",
    "\n",
    "    h = 1  # step size\n",
    "    sum = 0.5 * (np.log(1 + sorted_values[0]) / np.log(max_error) + np.log(\n",
    "        1 + sorted_values[-1]) / np.log(max_error))  # initial area\n",
    "\n",
    "    for i in range(1, size - 1):\n",
    "        sum += np.log(1 + sorted_values[i]) / np.log(max_error)\n",
    "\n",
    "    auc = sum * h / (size - 1)\n",
    "    return auc\n",
    "\n",
    "sorted_error_distances = sorted(\n",
    "    all_error_distances)  # assuming error_distances is a dictionary with error error_distances\n",
    "auc = calculate_auc(sorted_error_distances)\n",
    "print(f\"AUC: {auc}\")\n",
    "\n",
    "# Mean error distance\n",
    "mean_error_distance = np.mean(all_error_distances)\n",
    "print(f\"Mean error distance: {mean_error_distance}\")\n",
    "\n",
    "# Median error distance\n",
    "median_error_distance = np.median(all_error_distances)\n",
    "print(f\"Median error distance: {median_error_distance}\")\n",
    "\n",
    "# 𝜁 Median Symmetric Accuracy\n",
    "abs_all_log_q = np.abs(all_log_q)\n",
    "all_log_q = np.array(all_log_q)\n",
    "zeta = 100 * (np.exp(np.median(abs_all_log_q)) - 1)\n",
    "print(f\"Median Symmetric Accuracy: {zeta}\")\n",
    "\n",
    "# SSPB\n",
    "sspb = 100 * np.sign(np.median(all_log_q)) * (np.exp(np.abs(np.median(all_log_q))) - 1)\n",
    "print(f\"Symmetric Signed Percentage Bias: {sspb}\")\n",
    "\n",
    "print(f\"nof very off area predictions: {len(very_off_area_predictions)}\")\n"
   ],
   "id": "725eb8a109ed0fec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "average_precision = np.sum([evaluation[\"area_intersection\"]/evaluation['hexagon_area_estimate'] for evaluation in evaluation_results])/len(evaluation_results)\n",
    "average_recall = np.sum([evaluation[\"area_intersection\"]/evaluation['gt']['bb area'] for evaluation in evaluation_results])/len(evaluation_results)\n",
    "average_f1 = 2*average_precision*average_recall/(average_precision+average_recall)\n",
    "\n",
    "print(f\"average precision: {average_precision}, \\naverage recall: {average_recall}, \\naverage f1: {average_f1}\")"
   ],
   "id": "ebc5a29726be6539",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculate upper bound for F1 score",
   "id": "94e0a0b7c40e76e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_hexagon(side_length=1):\n",
    "    \"\"\"Create a regular hexagon centered at the origin with a given side length.\"\"\"\n",
    "    angles = np.linspace(0, 2 * np.pi, 7)[:-1]  # Six angles\n",
    "    vertices = [(side_length * np.cos(a), side_length * np.sin(a)) for a in angles]\n",
    "    return Polygon(vertices)\n",
    "\n",
    "def create_rectangle(width, height):\n",
    "    \"\"\"Create a rectangle centered at the origin with given width and height.\"\"\"\n",
    "    half_w, half_h = width / 2, height / 2\n",
    "    vertices = [(-half_w, -half_h), (half_w, -half_h), (half_w, half_h), (-half_w, half_h)]\n",
    "    return Polygon(vertices)\n",
    "\n",
    "def compute_f1(hexagon, rectangle):\n",
    "    \"\"\"Compute F1 score based on intersection, precision, and recall.\"\"\"\n",
    "    intersection = hexagon.intersection(rectangle).area\n",
    "    precision = intersection / hexagon.area\n",
    "    recall = intersection / rectangle.area if rectangle.area > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def simulate_f1():\n",
    "    \"\"\"Simulate the F1 score as the rectangle shrinks from the hexagon's bounding box and visualize with a heat map.\"\"\"\n",
    "    hexagon = create_hexagon()\n",
    "    bbox = hexagon.bounds  # (minx, miny, maxx, maxy)\n",
    "    max_width, max_height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "\n",
    "    max_overall = max(max_height, max_width)\n",
    "\n",
    "    width_values = height_values = np.linspace(0.1, max_overall, 1000)\n",
    "\n",
    "    f1_matrix = np.zeros((len(height_values), len(width_values)))\n",
    "    best_f1, best_w, best_h = 0, 0, 0\n",
    "\n",
    "    for i, h in enumerate(height_values):\n",
    "        for j, w in enumerate(width_values):\n",
    "            rectangle = create_rectangle(w, h)\n",
    "            f1_score = compute_f1(hexagon, rectangle)\n",
    "            f1_matrix[i, j] = f1_score\n",
    "            if f1_score > best_f1:\n",
    "                best_f1, best_w, best_h = f1_score, w, h\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(f1_matrix, extent=[0.1, max_overall, 0.1, max_overall], origin='lower', aspect='auto', cmap='coolwarm')\n",
    "    plt.colorbar(label='F1 Score')\n",
    "    plt.xlabel('Rectangle Width')\n",
    "    plt.ylabel('Rectangle Height')\n",
    "    plt.title('F1 Score Heat Map for Hexagon-Rectangle Overlap')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the hexagon and the best rectangle\n",
    "    best_rectangle = create_rectangle(best_w, best_h)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    x, y = hexagon.exterior.xy\n",
    "    ax.plot(x, y, 'b-', label='Hexagon')\n",
    "\n",
    "    x, y = best_rectangle.exterior.xy\n",
    "    ax.plot(x, y, 'r-', label=f'Best Rectangle (w={best_w:.3f}, h={best_h:.3})')\n",
    "\n",
    "    ax.set_xlim(-max_overall / 2, max_overall / 2)\n",
    "    ax.set_ylim(-max_overall / 2, max_overall / 2)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title('Best F1 Score Rectangle within Hexagon')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    return f1_matrix, best_f1\n",
    "\n",
    "# f1_matrix, best_f1 = simulate_f1()"
   ],
   "id": "a2ebc99c39625269",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
